---
{"dg-publish":true,"permalink":"/personal-page/final-year-project/"}
---



#private
<!--
- **Link to** [G2Net Detecting Continuous Gravitational Waves | Kaggle](https://www.kaggle.com/competitions/g2net-detecting-continuous-gravitational-waves/overview)
- **Tags** #机器学习 #FYP
- **Reference** 
	- [GitHub - rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models#) -->

## Context

G2Net is a network of Gravitational Wave, Geophysics and Machine Learning scientists. At present only signals from merging black holes and neutron stars have been detected. There are potentially many continuous signals from neutron stars in our own galaxy. Among those remaining are continuous gravitational-wave signals. By helping G2Net in this challenge you'll enable scientists to improve their sensitivity. 

---

## Goals
To find continuous gravitational wave signals. 

Each data sample contains either **real or simulated noise**and possibly a simulated continuous gravitational-wave signal (CW) . The task is to identify**when a signal is present in the data (target=1).**

---

## Details
- Dataset 
	- Real Data
	- Data Generation 
- ML Method
	- Tabular Feature with TabNet 
	- Image Feature with ViT with CNN and EfficientNet (Pre-trained)
- challenges 
	- Noise 
	- Sample imbalance 
	- Human influence when label
---

### Dataset
#### Real Data
Real dataset is from two gravitational-wave interferometers provided by G2Net.
> This is a spectrogram contains a true gravitational-wave clearly with label 1 and 0.  
<!-- element style="width:100%"-->

![CleanShot2022-11-09at12.40.48@2x](https://tuchuang-1303124258.cos.ap-shanghai.myqcloud.com/uPic/CleanShot%202022-11-09%20at%2012.40.48@2x.png)
![CleanShot2022-11-09at12.40.10@2x](https://tuchuang-1303124258.cos.ap-shanghai.myqcloud.com/uPic/CleanShot%202022-11-09%20at%2012.40.10@2x.png)

Actually, numerous of images are full of noise and cannot identify by our eyes. 

#### Data Generation
> [!information] 
> There are less more than DS can utilize time-frequency data from two gravitational-wave interferometers (LIGO Hanford & LIGO Livingston). So we need more data generated by ourselves to enhance model infer. 
> Each sample is comprised of a set of Short-time Fourier Transforms (SFTs) and corresponding GPS time stamps for each interferometer.
> For image Classification, we use spectrogram. 
<!-- element style="width:90%"-->

<!--
- **Link to** 
	- [G2Net-[TF]-[ViT] | Kaggle](https://www.kaggle.com/code/lau01b/g2net-tf-vit/notebook#Convolutional-Neural-Network)
	- [G2Net EDA:Data size/span/time-gap | Kaggle](https://www.kaggle.com/code/konomuabe/g2net-eda-data-size-span-time-gap)
	- [Basic spectrogram image classification](https://www.kaggle.com/code/dylanhedded/basic-spectrogram-image-classification/edit) --> 

**Read Dataset**
This is unusual as we most of the time see data splits as 80:20 but in our case it is 1: 16 which indicates that the competition creators are encouraging participants to generate their own data.

![CleanShot2022-11-09at12.42.30@2x](https://tuchuang-1303124258.cos.ap-shanghai.myqcloud.com/uPic/CleanShot%202022-11-09%20at%2012.42.30@2x.png)

**For data generation** 

> This part is from *Riroriro* proposed on 2019 by Buskirk and Babiuc-Hamilton. 

We can capture a kind of signal (simulate). 

![CleanShot2022-11-09at12.52.52@2x](https://tuchuang-1303124258.cos.ap-shanghai.myqcloud.com/uPic/CleanShot%202022-11-09%20at%2012.52.52@2x.png)

Then we use Constant-Q to visualize 
![CleanShot2022-11-09at12.53.09@2x](https://tuchuang-1303124258.cos.ap-shanghai.myqcloud.com/uPic/CleanShot%202022-11-09%20at%2012.53.09@2x.png)


Standard CW signals can be parameterised in terms of two sets of parameters:
- the Doppler-modulation parameters $\lambda$ 
- the amplitude parameters 

对于由快速旋转和孤立的中子星 (NS) 发射的 CW, 多普勒调制参数包括频率 F0 和线性平移参数 F1, 两者均取自参考时间 tref, 以及以赤道坐标的赤经角 Alpha 和赤纬角 Delta 为单位的天空位置. 另一方面, 振幅参数包括 CW 信号的平均振幅 h0, 信号的初始相位 phi、偏振角 psi 和 (余弦) 源的倾斜角 cosi, 这给我们提供了 NS 相对于探测器的相对方向.

为了产生噪声, 需要指定一组检测器（本例中为 H1 或 L1, 样本的持续时间和噪声的振幅谱密度 sqrtSX.

生成的 Noise 数据被保存为如下的短傅里叶变换 (Short Fourier Transforms (SFTs)的列表. 

![I3bCjw](https://tuchuang-1303124258.cos.ap-shanghai.myqcloud.com/uPic/I3bCjw.jpg)


## Method

### LGBM Features
<!--
- **Link to** 
	- [g2net : prepare features | Kaggle](https://www.kaggle.com/code/ahmedelfazouan/g2net-prepare-features) 
	- [g2net LGBM with smote and enn | Kaggle](https://www.kaggle.com/code/aspiring/g2net-lgbm-with-smote-and-enn)
	- [✅ Explore the Training Data Files 📂 | Kaggle](https://www.kaggle.com/code/ryanluoli2/explore-the-training-data-files) -->

Origin Data is HDF5 Files. **`hdf5`** stands for **Hierarchical Data Format version 5**. Each hierarchy with metadata. 

![M5wWSy](https://tuchuang-1303124258.cos.ap-shanghai.myqcloud.com/uPic/M5wWSy.jpg)

- HDF5
	- groups 
		- groups 
			- Dataset 
**For our data, one sample**
- KeysViewHDF5 [groups]
	- HDF5 group (3 members )
		- KeysView['H1', 'L1', 'frequency_Hz']
		- H1
			- SFTs
			- Timestamps
		- L1
			- SFTs
			- Timestamps
		- Frequency_HZ
			- Values

For id `001121a05` 
![[CleanShot 2022-11-09 at 13.04.16@2x 1.png\|CleanShot 2022-11-09 at 13.04.16@2x 1.png]]

For H1_SFTs[0:2] 
```python
array([[-2.0178011e-24+1.7066067e-22j, -1.4645843e-23+9.6421383e-23j,
         1.3910385e-23-4.3923662e-23j, ...,
        -1.7177181e-23+1.2837293e-22j,  1.3892107e-22-8.2977672e-23j,
         7.2729580e-23+4.4559575e-23j],
       [-1.8711387e-22+1.0073502e-22j, -1.5109396e-22+9.3674566e-24j,
         5.0192982e-23-5.2098252e-23j, ...,
        -5.9722122e-23-1.0904593e-22j,  8.0738399e-23+1.6148876e-22j,
         1.4760140e-22+1.0795126e-22j]], dtype=complex64)

```

We just aggregate with randomness  to generate values in timestamps we need to create features. 

- 360 features with 1 target 
	- Contains SFTs information 
![[CleanShot 2022-11-09 at 13.07.32@2x.png\|CleanShot 2022-11-09 at 13.07.32@2x.png]]

Then use a **TabNet** to train our data then ensemble it to our main CV model. 

> [!information]
> TabNet is a Deep Neural Network for tabular data and was designed to learn in a similar way than decision tree based models, in order to have their benefits : **_interpretability_** and **_sparse feature selection_**. TabNet uses **_sequential attention to choose which features to reason from at each decision step_**, enabling interpretability and better learning (as the learning capacity is used for the most salient/important features). The feature selection is instancewise, so it can be different for each input.


### CV Features

The ViT with CNN model consists of multiple Transformer blocks.
- use MultiHeadAttention layer as a self-attention mechanism applied to the sequence of patches. 
- The Transformer blocks produce a [batch_size, num_patches, projection_dim] tensor, which is processed via a Dense head to produce the final output.



--- 

# Writing 

- Literature Review 
	- NetWork Part
		- LeNet 
		- AlexNet
		- InceptionNet
		- ResNet
		- MnasNet 
		- EfficientNet 
	- Tabular Learning Part 
- Methodology 
	- Data 


## Data 

To parameterize standard CW signals 
- Doppler-modulation parameters $\lambda$ : The fluctuation of signal frequent. 
	- the frequency ’F0’ 
	- the linear spin-down parameter ’F1’
	- 'F0' and 'F1' are taken at a reference time ’tref’, and the sky position in terms of the right ascension ’Alpha’ and declination ’Delta’ angles  of equatorial coordinates for a CW emitted by a rapidly-spinning and isolated neutron star (NS).
-  the amplitude parameters $\mathcal{A}$: The overall amplitude of a CW based on the source’s properties
	- The average amplitude of a CW signal 'h0'
	- initial phase 'phi'
	- polarization angle 'psi'
	- (the cosine of) the inclination angle of the  source ’cosi' :  relative orientation of the NS in relation to the detector 
	- The am